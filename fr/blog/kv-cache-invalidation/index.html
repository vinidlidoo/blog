<!doctype html><html lang=fr><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;frame-src platform.twitter.com giscus.app;img-src 'self' data: pub-94e31bf482a74272bb61e9559b598705.r2.dev;media-src 'self' pub-94e31bf482a74272bb61e9559b598705.r2.dev;form-action 'self' buttondown.com formspree.io;connect-src 'self';script-src 'self' platform.twitter.com giscus.app" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://vinidlidoo.github.io name=base><title>
vinidlidoo • Invalidation du cache KV</title><link href=https://vinidlidoo.github.io/img/favicon.png rel=icon type=image/png><link href="https://vinidlidoo.github.io/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://vinidlidoo.github.io/main.css?h=045c365e19a4d50a64bb" rel=stylesheet><link href="https://vinidlidoo.github.io/css/details.css?h=8559b4e109b168c7cfa0" rel=stylesheet><link href="https://vinidlidoo.github.io/css/giscus-fix.css?h=dc895b97fbaf0f0d9908" rel=stylesheet><link href="https://vinidlidoo.github.io/css/newsletter.css?h=c872153c9c367e0bee16" rel=stylesheet><link href="https://vinidlidoo.github.io/css/contact.css?h=572915e83e02eb90c290" rel=stylesheet><link href="https://vinidlidoo.github.io/skins/indigo_ingot.css?h=d429472afbb246441b1a" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Pourquoi supprimer du contexte d'une conversation LLM force un recalcul complet" name=description><meta content="Pourquoi supprimer du contexte d'une conversation LLM force un recalcul complet" property=og:description><meta content="Invalidation du cache KV" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale:alternate><link href=https://vinidlidoo.github.io/fr/blog/kv-cache-invalidation/ hreflang=fr rel=alternate><meta content=en_GB property=og:locale:alternate><link href=https://vinidlidoo.github.io/ja/blog/kv-cache-invalidation/ hreflang=ja rel=alternate><meta content=en_GB property=og:locale:alternate><link href=https://vinidlidoo.github.io/blog/kv-cache-invalidation/ hreflang=en rel=alternate><meta content=https://vinidlidoo.github.io/fr/blog/kv-cache-invalidation/ property=og:url><meta content=vinidlidoo property=og:site_name><noscript><link href=https://vinidlidoo.github.io/no_js.css rel=stylesheet></noscript><script src=https://vinidlidoo.github.io/js/initializeTheme.min.js></script><script defer src=https://vinidlidoo.github.io/js/themeSwitcher.min.js></script><script src="https://vinidlidoo.github.io/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><script defer src=https://vinidlidoo.github.io/js/lunr/lunrStemmerSupport.min.js></script><script defer src=https://vinidlidoo.github.io/js/lunr/lunr.fr.min.js></script><body><a href=#main-content id=skip-link>Skip to content</a><header><nav class=navbar><div class=nav-title><a class=home-title href=https://vinidlidoo.github.io/fr/>vinidlidoo</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://vinidlidoo.github.io/fr/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://vinidlidoo.github.io/fr/tags/>étiquettes </a><li><a class="nav-links no-hover-padding" href=https://vinidlidoo.github.io/fr/contact/>contact </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Press $SHORTCUT to open search" class="search-icon interactive-icon" title="Press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class=language-switcher><details class=dropdown><summary aria-label="Language selection" title="Language selection" aria-haspopup=true role=button><div class=language-switcher-icon></div></summary> <div class=dropdown-content role=menu>Français<a aria-label=English href=https://vinidlidoo.github.io/blog/kv-cache-invalidation/ lang=en role=menuitem>English</a><a aria-label=日本語 href=https://vinidlidoo.github.io/ja/blog/kv-cache-invalidation/ lang=ja role=menuitem>日本語</a></div></details><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content id=main-content><main><article class=h-entry><h1 class="p-name article-title">Invalidation du cache KV</h1><a class="u-url u-uid" href=https://vinidlidoo.github.io/fr/blog/kv-cache-invalidation/></a><ul class=meta><span class="hidden p-author h-card"> <a class=u-url href=https://vinidlidoo.github.io rel=author title=Vincent>Vincent</a> </span><li><time class=dt-published datetime=2026-01-07>7th Jan 2026</time><li title="1451 words"><span aria-hidden=true class=separator>•</span>8 min read<li class=tag><span aria-hidden=true class=separator>•</span>Étiquettes: <li class=tag><a class=p-category href=https://vinidlidoo.github.io/fr/tags/ai/>ai</a></ul><ul class="meta last-updated"><li><time class=dt-updated datetime=2026-01-08>Updated on 8th Jan 2026</time><li><span aria-hidden=true class=separator>•</span><a class=external href=https://github.com/vinidlidoo/vinidlidoo.github.io/commits/main/content/blog/kv-cache-invalidation.fr.md>See changes</a></ul><p class=p-summary hidden>Pourquoi supprimer du contexte d'une conversation LLM force un recalcul complet<section class="e-content body"><p><img alt="Prédiction du prochain token" src=/img/next_token_prediction.webp><p>Pendant les fêtes, j'ai eu une conversation avec un ami à propos de la mise en cache des prompts. L'intuition de chacun sur l'ingénierie de contexte est logique : si vous discutez avec ChatGPT ou Claude et que la conversation accumule du contexte non pertinent, le supprimer devrait aider le modèle à se concentrer. Meilleure précision, non ?<p>Oui, mais il y a un piège. Supprimer des tokens au milieu d'une conversation invalide le <strong>cache KV</strong> — un mécanisme clé qui accélère l'inférence des LLM. On ne perd pas juste un peu de calcul mis en cache ; on perd <strong>tout ce qui suit la modification</strong>. C'est pourquoi claude.ai, ChatGPT ou Claude Code ne modifient ou ne suppriment pas fréquemment les messages précédents<sup class=footnote-reference id=fr-1-1><a href=#fn-1>1</a></sup>. Comme l'a exprimé un PM de Claude Code <a href=https://x.com/trq212/status/2004026126889320668>sur Twitter</a> : « <em>Les agents de programmation seraient d'un coût prohibitif s'ils ne maintenaient pas le cache de prompt entre les tours.</em> » Cet article explique pourquoi.<h2 id=prediction-du-prochain-token>Prédiction du prochain token</h2><p>Les LLM génèrent du texte un token à la fois. Étant donné une séquence de tokens $t_1, \ldots, t_i$, le modèle prédit une distribution de probabilité sur le prochain token :<p>$$P(t_{i+1} | t_1, \ldots, t_i)$$<p>Pour générer une réponse, le modèle échantillonne cette distribution (probablement <em>Paris</em> dans la figure ci-dessus), ajoute le nouveau token au contexte, et répète. Chaque nouveau token nécessite une <strong>passe forward</strong> à travers tout le modèle, traitant le contexte complet.<h2 id=la-passe-forward-du-transformer>La passe forward du Transformer</h2><p>Les LLM modernes utilisent l'architecture Transformer. Voici le fameux diagramme de « Attention Is All You Need » :<p><img alt="Architecture Transformer" src=/img/transformer.png><p>La boîte grise marquée « Nx » à droite est un <strong>bloc décodeur</strong> — il est répété $L$ fois. Chaque bloc contient une attention multi-têtes masquée et un réseau feed-forward.<sup class=footnote-reference id=fr-2-1><a href=#fn-2>2</a></sup><p>Chaque token $t_i$ commence comme un vecteur d'embedding $x_i$. En traversant les blocs, ce vecteur se transforme. Appelons le vecteur pour la position $i$ après le bloc $\ell$ l'<strong>état caché</strong> $z_i^{(\ell)}$.<p>Chaque bloc alimente le suivant : $z_i^{(\ell)}$ devient l'entrée pour calculer $z_i^{(\ell+1)}$. Après $L$ blocs, l'état caché final $z_i^{(L)}$ sert à prédire $P(t_{i+1} | t_1, \ldots, t_i)$, c'est-à-dire la distribution de probabilité du début.<h2 id=le-cache-kv>Le cache KV</h2><p>L'attention multi-têtes masquée dans chaque bloc calcule trois vecteurs à partir de chaque état caché $z_i^{(\ell)}$ — pour chaque position $i$, chaque bloc $\ell$, et chaque tête d'attention $h$ (<a href=https://huggingface.co/blog/llama31>Llama 3.1 405B</a> a 126 blocs et 128 têtes) :<ul><li><strong>Query</strong> $Q(z_i^{(\ell)})$ : que cherche la position $i$ ?<li><strong>Key</strong> $K(z_j^{(\ell)})$ : que contient la position $j$ ?<li><strong>Value</strong> $V(z_j^{(\ell)})$ : quelle information fournit la position $j$ ?</ul><p>La position $i$ prête attention à toutes les positions $j \leq i$ en comparant sa query à leurs keys, puis en prenant une somme pondérée de leurs values. Cela signifie que $z_i^{(\ell)}$, ainsi que Q, K, V, dépendent de <em>tous les tokens précédents</em>, pas seulement de $t_i$ seul.<p>Le cache KV exploite une observation clé : lors de la génération de <strong>nouveaux tokens</strong>, les vecteurs K et V des positions précédentes ne changent pas. On les met donc en cache. Pour chaque nouveau token, on calcule ses Q, K, V, puis on réutilise les K et V en cache pour l'attention. Cela transforme un travail $O(n^2)$ par token en $O(n)$.<h2 id=pourquoi-supprimer-des-tokens-casse-le-cache>Pourquoi supprimer des tokens casse le cache</h2><p>Considérons maintenant la suppression d'un token à la position $j$. Que se passe-t-il pour les vecteurs K et V en cache ? Supprimez le token $j$, et chaque état caché $z_{j+1}^{(\ell)}, z_{j+2}^{(\ell)}, \ldots$ change — ils prêtaient tous attention à la position $j$, mais ce n'est plus le cas. Selon la section précédente, des états cachés modifiés signifient des vecteurs K et V modifiés. Tout le cache à partir de la position $j$ est désormais obsolète.<h2 id=implications>Implications</h2><p><strong>La mise en cache des prompts nécessite une correspondance exacte du préfixe.</strong> Les fournisseurs d'API comme Anthropic et OpenAI mettent en cache l'état KV des prompts. Si votre nouvelle requête partage un préfixe exact avec une précédente, ils peuvent réutiliser le cache. Mais si vous modifiez quoi que ce soit — même un seul token au milieu — le cache est inutilisable à partir de ce point.<p><strong>L'invalidation du cache coûte cher.</strong> Imaginez modifier un token au début d'une conversation de 50 000 tokens. Chaque position après la modification doit voir ses vecteurs K et V recalculés — à travers tous les blocs et toutes les têtes. Cela représente plus de 800 millions de calculs de vecteurs pour Llama 3.1 405B. Le <a href=https://platform.claude.com/docs/en/build-with-claude/prompt-caching>prompt caching</a> d'Anthropic facture les succès de cache à 10 % du coût de base des tokens d'entrée ; un échec de cache signifie payer le plein tarif. La latence en souffre aussi : les succès de cache peuvent réduire le temps jusqu'au premier token de <a href=https://claude.com/blog/prompt-caching>jusqu'à 85 %</a> pour les prompts longs.<p><strong>On peut ajouter, mais pas modifier.</strong> Ajouter des tokens à la fin est peu coûteux : on étend simplement le cache. Insérer ou supprimer au milieu force le recalcul de tout ce qui suit. C'est pourquoi l'historique des conversations dans les chatbots tend à croître de façon monotone.<p><strong>Le compromis précision-coût est réel.</strong> Supprimer du contexte non pertinent peut améliorer la concentration du modèle, mais on paie en calcul. Pour les longues conversations, ce coût peut être substantiel. Parfois ça en vaut la peine ; souvent non. Une approche : <a href=https://forum.letta.com/t/breaking-prompt-caching/149>Letta suggère</a> de modifier les prompts de façon asynchrone pendant les périodes d'inactivité (via des « sleep-time agents »), pour que la reconstruction du cache se fasse quand l'utilisateur n'attend pas.<hr><h2 id=annexe-mathematiques-du-transformer>Annexe : Mathématiques du Transformer</h2><details><summary>Dérivation complète</summary> <p>La notation ici correspond à celle utilisée ci-dessus.</p> <h3 id=notation>Notation</h3> <ul><li>$V$ = taille du vocabulaire<li>$d$ = dimension du modèle (taille de l'embedding)<li>$k$ = dimension par tête (typiquement $k = d / H$)<li>$H$ = nombre de têtes d'attention<li>$m$ = dimension cachée du FFN (typiquement $4d$)<li>$n$ = longueur de la séquence<li>$L$ = nombre de blocs décodeurs</ul> <h3 id=etape-1-embeddings-des-tokens-d-entree>Étape 1 : Embeddings des tokens d'entrée</h3> <p>$$x_i = E[t_i] + p_i, \quad E \in \mathbb{R}^{V \times d}, \quad p_i \in \mathbb{R}^d$$</p> <p>où $t_i$ est l'indice du token et $p_i$ est l'encodage positionnel.</p> <p>Soit $X^{(0)} = [x_1, \dots, x_n] \in \mathbb{R}^{d \times n}$ l'entrée initiale des blocs transformer.</p> <h3 id=etapes-2-6-bloc-decodeur-repete-l-fois>Étapes 2-6 : Bloc décodeur (répété L fois)</h3> <p>Pour le bloc $\ell = 1, \dots, L$, avec entrée $X^{(\ell-1)} \in \mathbb{R}^{d \times n}$ :</p> <p><strong>Attention multi-têtes masquée</strong></p> <p>Queries, keys et values pour la tête $h$ :</p> <p>$$Q^{(h)}(x_i) = (W_h^{Q})^T x_i, \quad K^{(h)}(x_i) = (W_h^{K})^T x_i, \quad V^{(h)}(x_i) = (W_h^{V})^T x_i$$</p> <p>où $W_h^{Q}, W_h^{K}, W_h^{V} \in \mathbb{R}^{d \times k}$.</p> <p><strong>Poids d'attention masqués</strong></p> <p>$$\alpha_{i,j}^{(h)} = softmax_j \left(\frac{Q^{(h)}(x_i) \cdot K^{(h)}(x_j)}{\sqrt{k}} + M_{i,j}\right)$$</p> <p>où le masque causal $M_{i,j} = 0$ si $j \leq i$, et $M_{i,j} = -\infty$ si $j > i$.</p> <p><strong>Sortie pour chaque tête</strong></p> <p>$$u_i^{(h)} = \sum_{j=1}^{i} \alpha_{i,j}^{(h)} V^{(h)}(x_j) \in \mathbb{R}^{k}$$</p> <p><strong>Sortie concaténée</strong></p> <p>$$u_i' = \sum_{h=1}^{H} (W_h^{O})^T u_i^{(h)}, \quad W_h^{O} \in \mathbb{R}^{k \times d}$$</p> <p><strong>Résidu + LayerNorm</strong></p> <p>$$u_i = \text{LayerNorm}(x_i + u_i'; \gamma_1, \beta_1)$$</p> <h3 id=etapes-7-8-reseau-feed-forward>Étapes 7-8 : Réseau feed-forward</h3> <p>Pour chaque position $i$ :</p> <p>$$z_i' = (W_2)^T \text{ReLU}((W_1)^T u_i), \quad W_1 \in \mathbb{R}^{d \times m}, , W_2 \in \mathbb{R}^{m \times d}$$</p> <p><strong>Résidu + LayerNorm (sortie du bloc)</strong></p> <p>$$z_i = \text{LayerNorm}(u_i + z_i'; \gamma_2, \beta_2)$$</p> <p>Soit $X^{(\ell)} = [z_1, \dots, z_n]$. Ceci devient l'entrée du bloc $\ell + 1$.</p> <h3 id=etapes-9-10-logits-et-probabilites-de-sortie>Étapes 9-10 : Logits et probabilités de sortie</h3> <p>Après $L$ blocs, soit $Z = X^{(L)}$ les représentations finales.</p> <p>$$\text{logits}_i = E z_i + b, \quad E \in \mathbb{R}^{V \times d}, , b \in \mathbb{R}^V$$</p> <p>où $E$ est souvent lié aux embeddings d'entrée.</p> <p><strong>Probabilités de prédiction</strong></p> <p>$$P(t_{i+1} | t_1, \dots, t_i) = \text{softmax}(\text{logits}_i)$$</p> <p>La sortie à la position $i$ prédit le prochain token $t_{i+1}$, en utilisant uniquement l'information des tokens $t_1, \dots, t_i$ grâce au masquage causal.</p></details><hr><p><strong>Références :</strong><ul><li><a href=https://johnthickstun.com/docs/transformers.pdf>Transformer Notes</a> par John Thickstun<li><a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need</a> (Vaswani et al., 2017)</ul><hr><section class=footnotes><ol class=footnotes-list><li id=fn-1><p>La compaction se produit, mais rarement. <a href=#fr-1-1>↩</a></p><li id=fn-2><p>Le diagramme montre l'architecture encodeur-décodeur originale. Les LLM modernes comme GPT et Claude sont <em>décodeur uniquement</em> : ils omettent le côté gauche (encodeur) et la « Multi-Head Attention » du milieu qui prête attention aux sorties de l'encodeur. <a href=#fr-2-1>↩</a></p></ol></section></section><p class=credit><em>Cet article a été écrit en collaboration avec <a href=https://claude.ai>Claude</a>.</em><form action=https://buttondown.com/api/emails/embed-subscribe/vinidlidoo class=embeddable-buttondown-form method=post target=_blank><label for=bd-email>Recevez mes nouveaux articles par email.</label><input id=bd-email name=email placeholder=your@email.com type=email><input value="S'abonner" type=submit></form></article></main><link href=https://vinidlidoo.github.io/katex.min.css rel=stylesheet><script defer src=https://vinidlidoo.github.io/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://vinidlidoo.github.io/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" rel=" me" href=https://github.com/vinidlidoo> <img alt=github loading=lazy src=https://vinidlidoo.github.io/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://x.com/vinidlidoo> <img alt=x loading=lazy src=https://vinidlidoo.github.io/social_icons/x.svg title=x> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> <p><p>© 2026 Vincent Ethier</p> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> 1 result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>