<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;frame-src platform.twitter.com giscus.app;img-src 'self' data: pub-94e31bf482a74272bb61e9559b598705.r2.dev;media-src 'self' pub-94e31bf482a74272bb61e9559b598705.r2.dev;form-action 'self' buttondown.com formspree.io;connect-src 'self';script-src 'self' platform.twitter.com giscus.app" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://vinidlidoo.github.io name=base><title>
vinidlidoo • When Parquet Files Beat CSV</title><link href=https://vinidlidoo.github.io/img/favicon.png rel=icon type=image/png><link title="vinidlidoo - Atom Feed" href=https://vinidlidoo.github.io/atom.xml rel=alternate type=application/atom+xml><link href="https://vinidlidoo.github.io/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://vinidlidoo.github.io/main.css?h=045c365e19a4d50a64bb" rel=stylesheet><link href="https://vinidlidoo.github.io/css/details.css?h=8559b4e109b168c7cfa0" rel=stylesheet><link href="https://vinidlidoo.github.io/css/giscus-fix.css?h=dc895b97fbaf0f0d9908" rel=stylesheet><link href="https://vinidlidoo.github.io/css/newsletter.css?h=c872153c9c367e0bee16" rel=stylesheet><link href="https://vinidlidoo.github.io/css/contact.css?h=572915e83e02eb90c290" rel=stylesheet><link href="https://vinidlidoo.github.io/css/details.css?h=8559b4e109b168c7cfa0" rel=stylesheet><link href="https://vinidlidoo.github.io/skins/indigo_ingot.css?h=d429472afbb246441b1a" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="The physical reality that makes file layout matter" name=description><meta content="The physical reality that makes file layout matter" property=og:description><meta content="When Parquet Files Beat CSV" property=og:title><meta content=article property=og:type><meta content="https://vinidlidoo.github.io/img/row-vs-column-orientation.webp?h=00928d4da4f88e2456da" property=og:image><meta content=1200 property=og:image:width><meta content=670 property=og:image:height><meta content="https://vinidlidoo.github.io/img/row-vs-column-orientation.webp?h=00928d4da4f88e2456da" name=twitter:image><meta content=summary_large_image name=twitter:card><meta content=en_GB property=og:locale:alternate><link href=https://vinidlidoo.github.io/ja/blog/understanding-parquet-files/ hreflang=ja rel=alternate><meta content=en_GB property=og:locale:alternate><link href=https://vinidlidoo.github.io/blog/understanding-parquet-files/ hreflang=en rel=alternate><meta content=en_GB property=og:locale:alternate><link href=https://vinidlidoo.github.io/fr/blog/understanding-parquet-files/ hreflang=fr rel=alternate><meta content=https://vinidlidoo.github.io/blog/understanding-parquet-files/ property=og:url><meta content=vinidlidoo property=og:site_name><noscript><link href=https://vinidlidoo.github.io/no_js.css rel=stylesheet></noscript><script src=https://vinidlidoo.github.io/js/initializeTheme.min.js></script><script defer src=https://vinidlidoo.github.io/js/themeSwitcher.min.js></script><script src="https://vinidlidoo.github.io/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><a href=#main-content id=skip-link>Skip to content</a><header><nav class=navbar><div class=nav-title><a class=home-title href=https://vinidlidoo.github.io/>vinidlidoo</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://vinidlidoo.github.io/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://vinidlidoo.github.io/tags/>tags </a><li><a class="nav-links no-hover-padding" href=https://vinidlidoo.github.io/contact/>contact </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Press $SHORTCUT to open search" class="search-icon interactive-icon" title="Press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class=language-switcher><details class=dropdown><summary aria-label="Language selection" title="Language selection" aria-haspopup=true role=button><div class=language-switcher-icon></div></summary> <div class=dropdown-content role=menu>English<a aria-label=日本語 href=https://vinidlidoo.github.io/ja/blog/understanding-parquet-files/ lang=ja role=menuitem>日本語</a><a aria-label=Français href=https://vinidlidoo.github.io/fr/blog/understanding-parquet-files/ lang=fr role=menuitem>Français</a></div></details><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content id=main-content><main><article class=h-entry><h1 class="p-name article-title">When Parquet Files Beat CSV</h1><a class="u-url u-uid" href=https://vinidlidoo.github.io/blog/understanding-parquet-files/></a><ul class=meta><span class="hidden p-author h-card"> <a class=u-url href=https://vinidlidoo.github.io rel=author title=Vincent>Vincent</a> </span><li><time class=dt-published datetime=2026-01-23>23rd Jan 2026</time><li title="1992 words"><span aria-hidden=true class=separator>•</span>10 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://vinidlidoo.github.io/tags/computer-science/>computer-science</a></ul><ul class="meta last-updated"><li><time class=dt-updated datetime=2026-02-18>Updated on 18th Feb 2026</time><li><span aria-hidden=true class=separator>•</span><a class=external href=https://github.com/vinidlidoo/vinidlidoo.github.io/commits/main/content/blog/understanding-parquet-files.md>See changes</a></ul><p class=p-summary hidden>The physical reality that makes file layout matter<section class="e-content body"><p><img alt="Row-oriented vs column-oriented storage" src=/img/row-vs-column-orientation.webp><p>Back in December 2021, I was leading a new team at Amazon building a trend analytics application. We had data flowing into S3 as CSV files, getting ingested into a database, feeding weekly batch jobs. A data engineer proposed switching the storage format from CSV to Parquet. A debate ensued. Parquet won.<p>I'll be honest: I never deeply understood <em>why</em>. When I pressed for reasons, I heard that columnar storage was better for performance, offered better compression, and so on. It felt a bit too good to be true. I didn't have a firm grasp of the trade-offs, let alone the mechanics behind the benefits. It was my first 90 days into the role, so I did what many managers do: went with my gut and moved on. This post is my attempt to finally get it.<h2 id=files-as-byte-arrays>Files as Byte Arrays</h2><p>On disk, a file's data is stored as a contiguous <strong>sequence of bytes</strong>:<sup class=footnote-reference id=fr-1-1><a href=#fn-1>1</a></sup> $[b_0, b_1, b_2, \ldots, b_n]$ where each $b_i$ is a byte (8 bits, each 0 or 1) and $n$ typically ranges from millions (MB) to billions (GB) for analytics workloads.<p>Analytics queries rarely need all this data. A typical query might aggregate one column, filter on another, and ignore the rest. If your file has 100 columns and 10 million rows, but your query only touches 3 columns, reading the entire file means transferring 30x more bytes than necessary. At scale—hundreds of files, each gigabytes—this overhead dominates. Reading entire files is not viable.<p>So you need to be surgical: extract only the bytes you actually need.<p>Two operations let you do this:<ul><li><strong>seek</strong>: position the read head to byte $b_i$<li><strong>read</strong>: transfer bytes sequentially from $b_i$ onward</ul><p>The file <strong>layout</strong> determines whether the data you need is contiguous (one seek) or scattered (many seeks).<p>But there's a constraint: <strong>seek is expensive relative to read</strong>. A traditional hard drive has ~10ms access latency (the seek) and 150 MB/s throughput (the read). Compare:<ul><li>Reading 10 bytes: 10ms + ~0ms = <strong>10ms</strong><li>Reading 1MB: 10ms + 6.7ms = <strong>17ms</strong></ul><p>Going from 10 bytes to 1MB (100,000x more data) doesn't even double the I/O time if the data being read is contiguous. The goal is clear: <strong>minimize seeks, maximize bytes per seek</strong>. The strategy that achieves this is called <strong>batching</strong>: read large contiguous chunks instead of many small reads scattered across the file.<p>The same principle applies to cloud object storage like S3. AWS's disks still have seek overhead, but from your perspective the bottleneck is HTTP request overhead (TCP, TLS, round-trip). Batching here means requesting large byte ranges per HTTP request. Unlike disk (one read head), S3 lets you issue multiple requests in parallel, but concurrency is limited so the goal remains the same: <strong>fewer requests with larger byte ranges</strong>.<div class="table-wrapper table-wide"><table><thead><tr><th>Storage<th>Access Latency<th>Throughput<th>Implication<tbody><tr><td>HDD<td>~10ms (mechanical seek)<td>150 MB/s<td>Latency dominates; batching essential<tr><td>SSD<sup class=footnote-reference id=fr-2-1><a href=#fn-2>2</a></sup><td>~0.1ms (no moving parts)<td>500–3000 MB/s<td>Smaller penalty per seek; batching still wins<tr><td>S3<td>~100ms (HTTP round-trip)<td>100+ MB/s<td>Large byte ranges per request; parallelize across chunks</table></div><h2 id=row-vs-column-orientation>Row vs Column Orientation</h2><p>Analytics data is typically tabular: rows and columns. When you serialize a table into a byte sequence, there are two natural choices. Consider a simple employee table:<div class=table-wrapper><table><thead><tr><th>name<th>age<th>salary<th>dept<tbody><tr><td>Alice<td>32<td>95000<td>Eng<tr><td>Bob<td>28<td>72000<td>Mkt<tr><td>Carol<td>45<td>120000<td>Eng</table></div><p><strong>Row-oriented</strong> (CSV): store each row contiguously, then the next row. <code>[Alice,32,95000,Eng][Bob,28,72000,Mkt][Carol,45,120000,Eng]</code><p><strong>Column-oriented</strong> (Parquet): store each column contiguously, then the next column. <code>[Alice,Bob,Carol][32,28,45][95000,72000,120000][Eng,Mkt,Eng]</code><p>This changes which bytes you need to read. Consider <code>SELECT name, salary</code>: you need 2 of 4 columns.<p>With CSV, columns are interleaved within each row. You could read the entire file and discard what you don't need, but we just established that's not viable at scale. What if you had an index telling you exactly where each field starts? Could you then seek directly to name and salary and read just those?<p>You could, but it wouldn't help. To read 2 columns from 1 million rows, you'd need 2 million separate seeks (one per field). At 10ms per seek on HDD, that's 5+ hours of seek time alone. The problem isn't knowing where the data is. The problem is that the data you need is <em>scattered</em>. Row-oriented layout forces you to either read everything or make millions of tiny reads. Neither is acceptable.<p>Columnar layout solves this. Each column is stored contiguously, so reading name and salary means two seeks and two sequential reads. The data you need is physically together. You just need some way to locate where each column starts. That's what Parquet provides.<h2 id=parquet-file-structure>Parquet File Structure</h2><p>A Parquet file has three key components:<p><img alt="Parquet file structure" src=/img/parquet-file-structure.webp><p>As a byte sequence:<pre><code>[RG0:Col0][RG0:Col1][RG0:Col2]...[RG1:Col0][RG1:Col1][RG1:Col2]...[Footer]
</code></pre><p><strong>Row groups</strong> (~128MB each) are horizontal partitions of rows. They enable parallel processing: distributed query engines like Spark or BigQuery can assign different row groups to different workers.<p><strong>Column chunks</strong> live within each row group. Each column's data is stored contiguously. This is where columnar storage actually happens. Column chunks are further divided into <strong>pages</strong> (~1MB each), which is where encoding and compression are applied. We won't go into page-level details here.<p><strong>The footer</strong> is stored at the end of the file and contains the metadata you need to read surgically: the offset (where to seek), size (how much to read), and statistics (min/max/nulls) for every column chunk in every row group.<p>Here's what the footer looks like (simplified):<pre><code>Footer:
  Schema: name (STRING), age (INT32), salary (INT64), dept (STRING)

  Row Group 0 (rows 0–99,999):
    name:   offset=0,      size=2.1MB, min="Aaron",  max="Cynthia", nulls=0
    age:    offset=2.1MB,  size=0.4MB, min=18,       max=67,        nulls=12
    salary: offset=2.5MB,  size=0.8MB, min=31000,    max=185000,    nulls=0
    dept:   offset=3.3MB,  size=0.1MB, min="Design", max="Sales",   nulls=0

  Row Group 1 (rows 100,000–199,999):
    ...
</code></pre><p>To read a Parquet file, you first seek to the end, read the footer, then use it to locate exactly the data you need. This structure enables three key benefits: <strong>projection efficiency</strong> (read only the columns you need), <strong>compression</strong> (column chunks contain homogeneous data), and <strong>predicate pushdown</strong> (skip row groups based on statistics). There are additional benefits—parallelism from row groups and type safety from the schema—but these three account for most of why Parquet wins for analytics.<h3 id=1-projection-efficiency>1. Projection Efficiency</h3><p>Let's put concrete numbers to this. Consider 1 million employee records with 4 columns totaling ~100MB. The query <code>SELECT name, salary</code> needs only 2 columns.<p>Using the footer from our earlier example: name is at offset 0 (2.1MB), salary is at offset 2.5MB (0.8MB). Two seeks, 2.9MB transferred. On HDD, that's ~40ms total. You skip 97% of the file.<h3 id=2-compression>2. Compression</h3><p>Fewer bytes means even faster I/O. The columns you do read can be made smaller still.<p>Within each column chunk, all values share the same type, and in practice they often follow patterns: repeated categories, sequential timestamps, sorted keys. Parquet exploits these patterns through <strong>encoding</strong>: column-aware transformations applied at the page level.<p><strong>Dictionary encoding</strong> for low-cardinality strings (few unique values). Consider 8 department names repeated across 1M rows. Instead of storing "Engineering" 200k times (~12 bytes each), build a dictionary mapping each unique value to a small integer: <code>{0: "Design", 1: "Engineering", ...}</code>. Then store just the integer codes (1 byte each) instead of the full strings. ~12:1 compression.<p><strong>Delta encoding</strong> for sequential integers. Timestamps often increment by small amounts: <code>[1704067200, 1704067201, 1704067203, ...]</code>. Instead of storing each 8-byte value, store the first value once, then just the differences: <code>[1704067200, +1, +2, ...]</code>. Deltas fit in 1–2 bytes. ~4–8:1 compression.<p><strong>Run-length encoding (RLE)</strong> for consecutive repeated values. If data is sorted, you get long runs:<sup class=footnote-reference id=fr-3-1><a href=#fn-3>3</a></sup> <code>Design, Design, ...(50k times)..., Engineering, ...</code>. Instead of repeating the value, store it once with a count: <code>(Design, 50000), (Engineering, 200000), ...</code>. Compression scales with run length; a 50k run becomes a single (value, count) pair.<p>After encoding, generic <strong>compression</strong> (Snappy, Zstd, or others) is applied to the result for further reduction. Both benefit from columnar layout: <strong>grouping values by column exposes patterns that shrink file size</strong>.<h3 id=3-predicate-pushdown>3. Predicate Pushdown</h3><p>Predicate pushdown lets you skip entire row groups without reading them.<p>A <strong>predicate</strong> is a condition that filters rows: the <code>WHERE</code> clause in SQL. In a query execution plan, operations form a hierarchy—read data at the bottom, transform and filter higher up. "Pushdown" means moving the filter down that hierarchy, from the query engine to the storage layer. Instead of reading data and then discarding rows that don't match, you skip them before reading. The footer's min/max statistics make this possible: Parquet can check whether a row group could possibly contain matches without reading the actual data.<p>Query: <code>SELECT name FROM employees WHERE salary > 200000</code><ol><li>Read footer<li>Check salary statistics per row group: <ul><li>Row Group 0: salary max = 185,000 → <strong>skip</strong> (no row can match)<li>Row Group 1: salary max = 210,000 → <strong>read</strong> (might have matches)<li>Row Group 2: salary max = 178,000 → <strong>skip</strong><li>...</ul><li>Only read name and salary chunks from row groups that survived</ol><p>If 2 of 10 row groups survive, you've eliminated 80% of I/O before reading any actual data.<p>This works for strings too. Min/max use alphabetical ordering, so if a row group has min="Aaron" and max="Cynthia", a query for <code>name = 'Zoe'</code> can skip it entirely.<details><summary>Bloom filters for high-cardinality columns</summary> <p>For high-cardinality columns like <code>user_id</code>, min/max is useless (the range spans everything). Bloom filters offer an alternative: a bit array with multiple hash functions that answers "definitely not here" or "maybe here." A false positive ("maybe here" when the value isn't actually there) means a wasted read. The rate follows $(1 - e^{-kn/m})^k$ where $k$ is hash functions, $n$ is rows, $m$ is bits—and there's a closed-form formula for the optimal $k$ that minimizes this rate. A topic for another post.</p></details><h2 id=the-tradeoffs>The Tradeoffs</h2><p>Parquet optimizes for analytical reads: many rows, few columns. The costs show up in two places:<p><strong>Writes are expensive and inflexible.</strong> Creating a Parquet file requires buffering an entire row group in memory (~128MB), computing statistics for every column chunk, applying encoding, and compressing. CSV is just concatenating strings. And Parquet files are immutable: you cannot append rows without rewriting the file (the footer would be invalidated). With CSV, <code>echo "new,row" >> file.csv</code> just works.<p><strong>Not all reads benefit.</strong> Single-row lookups are terrible: even with predicate pushdown, you read entire column chunks (megabytes) to retrieve one row. Row-oriented databases use indexes for O(log n) single-record access. And the more columns you select, the less you gain. <code>SELECT *</code> reads everything, losing the projection benefit (though compression still helps), and pays reconstruction overhead to stitch columns back into rows.<p>If your workload is transactional (lots of single-record reads and writes), Parquet is the wrong choice.<h2 id=takeaway>Takeaway</h2><p>The format you choose should match your workload:<ul><li>Analytics (scan millions of rows, aggregate few columns, filter) → Parquet<li>Transactions (fetch/update/add single records by key) → row-oriented</ul><p>Many systems use both. Postgres for the live app, Parquet files (or a columnar warehouse like BigQuery) for reporting. They serve different purposes.<p>Parquet won the columnar analytics category so thoroughly that innovation moved to adjacent spaces: Arrow for in-memory processing, lakehouses (Delta Lake, Iceberg, Hudi) for transactions and appends on top of immutable files.<p>The underlying principle is the access latency asymmetry: whether it's disk seeks or HTTP round-trips, the cost of <em>starting</em> a read dominates the cost of <em>continuing</em> it. Organize your data so the bytes you need are contiguous, and you win.<hr><section class=footnotes><ol class=footnotes-list><li id=fn-1><p>A simplification: files can be fragmented across non-contiguous disk blocks, and filesystems add abstraction layers. The mental model still holds for understanding layout trade-offs. <a href=#fr-1-1>↩</a></p><li id=fn-2><p>SSDs eliminate mechanical seeks and are more forgiving, but the principle holds: few large sequential reads beat many small reads. <a href=#fr-2-1>↩</a></p><li id=fn-3><p>Parquet doesn't sort your data. You must sort before writing. The primary sort key benefits most; secondary keys benefit less, and only if low-cardinality. <a href=#fr-3-1>↩</a></p></ol></section></section><p class=credit><em>This post was written in collaboration with <a href=https://claude.ai>Claude</a>.</em><form action=https://buttondown.com/api/emails/embed-subscribe/vinidlidoo class=embeddable-buttondown-form method=post target=_blank><label for=bd-email>Get notified when I post something new.</label><input id=bd-email name=email placeholder=your@email.com type=email><input type=submit value=Subscribe></form></article></main><link href=https://vinidlidoo.github.io/katex.min.css rel=stylesheet><script defer src=https://vinidlidoo.github.io/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://vinidlidoo.github.io/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://vinidlidoo.github.io/atom.xml> <img alt=feed loading=lazy src=https://vinidlidoo.github.io/social_icons/rss.svg title=feed> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://github.com/vinidlidoo> <img alt=github loading=lazy src=https://vinidlidoo.github.io/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://x.com/vinidlidoo> <img alt=x loading=lazy src=https://vinidlidoo.github.io/social_icons/x.svg title=x> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> <p><p>© 2026 Vincent Ethier</p> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> 1 result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>