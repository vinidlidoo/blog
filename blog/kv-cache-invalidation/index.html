<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;frame-src platform.twitter.com giscus.app;img-src 'self' data: pub-94e31bf482a74272bb61e9559b598705.r2.dev;media-src 'self' pub-94e31bf482a74272bb61e9559b598705.r2.dev;form-action 'self' buttondown.com formspree.io;connect-src 'self';script-src 'self' platform.twitter.com giscus.app" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://vinidlidoo.github.io name=base><title>
vinidlidoo • KV Cache Invalidation</title><link href=https://vinidlidoo.github.io/img/favicon.png rel=icon type=image/png><link title="vinidlidoo - Atom Feed" href=https://vinidlidoo.github.io/atom.xml rel=alternate type=application/atom+xml><link href="https://vinidlidoo.github.io/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://vinidlidoo.github.io/main.css?h=045c365e19a4d50a64bb" rel=stylesheet><link href="https://vinidlidoo.github.io/css/details.css?h=8559b4e109b168c7cfa0" rel=stylesheet><link href="https://vinidlidoo.github.io/css/giscus-fix.css?h=dc895b97fbaf0f0d9908" rel=stylesheet><link href="https://vinidlidoo.github.io/css/newsletter.css?h=c872153c9c367e0bee16" rel=stylesheet><link href="https://vinidlidoo.github.io/css/contact.css?h=572915e83e02eb90c290" rel=stylesheet><link href="https://vinidlidoo.github.io/skins/indigo_ingot.css?h=d429472afbb246441b1a" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Why removing context from an LLM conversation forces full recomputation" name=description><meta content="Why removing context from an LLM conversation forces full recomputation" property=og:description><meta content="KV Cache Invalidation" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale:alternate><link href=https://vinidlidoo.github.io/fr/blog/kv-cache-invalidation/ hreflang=fr rel=alternate><meta content=en_GB property=og:locale:alternate><link href=https://vinidlidoo.github.io/ja/blog/kv-cache-invalidation/ hreflang=ja rel=alternate><meta content=en_GB property=og:locale:alternate><link href=https://vinidlidoo.github.io/blog/kv-cache-invalidation/ hreflang=en rel=alternate><meta content=https://vinidlidoo.github.io/blog/kv-cache-invalidation/ property=og:url><meta content=vinidlidoo property=og:site_name><noscript><link href=https://vinidlidoo.github.io/no_js.css rel=stylesheet></noscript><script src=https://vinidlidoo.github.io/js/initializeTheme.min.js></script><script defer src=https://vinidlidoo.github.io/js/themeSwitcher.min.js></script><script src="https://vinidlidoo.github.io/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><a href=#main-content id=skip-link>Skip to content</a><header><nav class=navbar><div class=nav-title><a class=home-title href=https://vinidlidoo.github.io/>vinidlidoo</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://vinidlidoo.github.io/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://vinidlidoo.github.io/tags/>tags </a><li><a class="nav-links no-hover-padding" href=https://vinidlidoo.github.io/contact/>contact </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Press $SHORTCUT to open search" class="search-icon interactive-icon" title="Press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class=language-switcher><details class=dropdown><summary aria-label="Language selection" title="Language selection" aria-haspopup=true role=button><div class=language-switcher-icon></div></summary> <div class=dropdown-content role=menu>English<a aria-label=Français href=https://vinidlidoo.github.io/fr/blog/kv-cache-invalidation/ lang=fr role=menuitem>Français</a><a aria-label=日本語 href=https://vinidlidoo.github.io/ja/blog/kv-cache-invalidation/ lang=ja role=menuitem>日本語</a></div></details><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content id=main-content><main><article class=h-entry><h1 class="p-name article-title">KV Cache Invalidation</h1><a class="u-url u-uid" href=https://vinidlidoo.github.io/blog/kv-cache-invalidation/></a><ul class=meta><span class="hidden p-author h-card"> <a class=u-url href=https://vinidlidoo.github.io rel=author title=Vincent>Vincent</a> </span><li><time class=dt-published datetime=2026-01-07>7th Jan 2026</time><li title="1316 words"><span aria-hidden=true class=separator>•</span>7 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://vinidlidoo.github.io/tags/ai/>ai</a></ul><ul class="meta last-updated"><li><time class=dt-updated datetime=2026-01-08>Updated on 8th Jan 2026</time><li><span aria-hidden=true class=separator>•</span><a class=external href=https://github.com/vinidlidoo/vinidlidoo.github.io/commits/main/content/blog/kv-cache-invalidation.md>See changes</a></ul><p class=p-summary hidden>Why removing context from an LLM conversation forces full recomputation<section class="e-content body"><p><img alt="Next token prediction" src=/img/next_token_prediction.webp><p>Over the holiday break, I had a conversation with a friend about prompt caching. Everyone's intuition about context engineering is sensible: if you're chatting with ChatGPT or Claude and the conversation accumulates irrelevant context, removing it should help the model focus. Better accuracy, right?<p>Yes, but there's a catch. Removing tokens from the middle of a conversation invalidates the <strong>KV cache</strong>—a key mechanism that speeds up LLM inference. You don't just lose a bit of cached work; you lose <strong>everything after the edit</strong>. This is why claude.ai, ChatGPT, or Claude Code don't frequently edit or delete earlier messages<sup class=footnote-reference id=fr-1-1><a href=#fn-1>1</a></sup>. As a Claude Code PM <a href=https://x.com/trq212/status/2004026126889320668>put it</a>: "<em>Coding agents would be cost prohibitive if they didn't maintain the prompt cache between turns.</em>" This post explains why.<h2 id=next-token-prediction>Next-Token Prediction</h2><p>LLMs generate text one token at a time. Given a sequence of tokens $t_1, \ldots, t_i$, the model predicts a probability distribution over the next token:<p>$$P(t_{i+1} | t_1, \ldots, t_i)$$<p>To generate a response, the model samples from this distribution (likely <em>Paris</em> in the figure above), appends the new token to the context, and repeats. Each new token requires a <strong>forward pass</strong> through the entire model, processing the full context.<h2 id=the-transformer-forward-pass>The Transformer Forward Pass</h2><p>Modern LLMs use the transformer architecture. Here's the famous diagram from "Attention Is All You Need":<p><img alt="Transformer architecture" src=/img/transformer.png><p>The grey box marked "Nx" to the right is a <strong>decoder block</strong>—it's repeated $L$ times. Each block contains a masked multi-head attention and a feed-forward network.<sup class=footnote-reference id=fr-2-1><a href=#fn-2>2</a></sup><p>Each token $t_i$ starts as an embedding vector $x_i$. As it passes through the blocks, this vector gets transformed. Call the vector for position $i$ after block $\ell$ the <strong>hidden state</strong> $z_i^{(\ell)}$.<p>Each block feeds into the next: $z_i^{(\ell)}$ becomes the input for computing $z_i^{(\ell+1)}$. After $L$ blocks, the final hidden state $z_i^{(L)}$ is used to predict $P(t_{i+1} | t_1, \ldots, t_i)$, that is, the probability distribution we started with.<h2 id=the-kv-cache>The KV Cache</h2><p>The masked multi-head attention in each block computes three vectors from each hidden state $z_i^{(\ell)}$—for every position $i$, every block $\ell$, and every attention head $h$ (<a href=https://huggingface.co/blog/llama31>Llama 3.1 405B</a> has 126 blocks and 128 heads):<ul><li><strong>Query</strong> $Q(z_i^{(\ell)})$: what is position $i$ looking for?<li><strong>Key</strong> $K(z_j^{(\ell)})$: what does position $j$ contain?<li><strong>Value</strong> $V(z_j^{(\ell)})$: what information does position $j$ provide?</ul><p>Position $i$ attends to all positions $j \leq i$ by comparing its query against their keys, then taking a weighted sum of their values. This means $z_i^{(\ell)}$, and Q, K, V, depend on <em>all preceding tokens</em>, not just $t_i$ alone.<p>The KV cache exploits a key observation: when generating <strong>new tokens</strong>, the K and V vectors for previous positions don't change. So we cache them. For each new token, we compute its Q, K, V, then reuse the cached K's and V's for attention. This turns $O(n^2)$ per-token work into $O(n)$.<h2 id=why-removing-tokens-breaks-the-cache>Why Removing Tokens Breaks the Cache</h2><p>Now consider removing a token from position $j$. What happens to the cached K and V vectors? Remove token $j$, and every hidden state $z_{j+1}^{(\ell)}, z_{j+2}^{(\ell)}, \ldots$ changes—they all attended to position $j$ but no longer do. Per previous section, changed hidden states mean changed K and V vectors. The entire cache from position $j$ onward is now stale.<h2 id=implications>Implications</h2><p><strong>Prompt caching requires exact prefix match.</strong> API providers like Anthropic and OpenAI cache the KV state for prompts. If your new request shares an exact prefix with a previous one, they can reuse the cache. But if you modify anything—even a single token in the middle—the cache is useless from that point onward.<p><strong>Cache invalidation is expensive.</strong> Consider editing a token early in a 50,000-token conversation. Every position after the edit needs its K and V vectors recomputed—across all blocks and heads. That's over 800 million vector computations for Llama 3.1 405B. Anthropic's <a href=https://platform.claude.com/docs/en/build-with-claude/prompt-caching>prompt caching</a> prices cache hits at 10% of base input token cost; a cache miss means paying the full price. Latency suffers too: cache hits can reduce time-to-first-token by <a href=https://claude.com/blog/prompt-caching>up to 85%</a> for long prompts.<p><strong>You can append, but you can't edit.</strong> Adding tokens to the end is cheap: just extend the cache. Inserting or deleting in the middle forces recomputation of everything downstream. This is why conversation history in chatbots tends to grow monotonically.<p><strong>The accuracy-cost tradeoff is real.</strong> Removing irrelevant context might improve model focus, but you pay with compute. For long conversations, this cost can be substantial. Sometimes it's worth it; often it's not. One approach: <a href=https://forum.letta.com/t/breaking-prompt-caching/149>Letta suggests</a> prompt edits asynchronously during idle periods ("via sleep-time agents"), so the cache reconstruction happens when the user isn't waiting.<hr><h2 id=appendix-transformer-math>Appendix: Transformer Math</h2><details><summary>Full derivation</summary> <p>The notation here matches what's used above.</p> <h3 id=notation>Notation</h3> <ul><li>$V$ = vocabulary size<li>$d$ = model dimension (embedding size)<li>$k$ = head dimension (typically $k = d / H$)<li>$H$ = number of attention heads<li>$m$ = FFN hidden dimension (typically $4d$)<li>$n$ = sequence length<li>$L$ = number of decoder blocks</ul> <h3 id=step-1-input-token-embeddings>Step 1: Input Token Embeddings</h3> <p>$$x_i = E[t_i] + p_i, \quad E \in \mathbb{R}^{V \times d}, \quad p_i \in \mathbb{R}^d$$</p> <p>where $t_i$ is the token index and $p_i$ is the positional encoding.</p> <p>Let $X^{(0)} = [x_1, \dots, x_n] \in \mathbb{R}^{d \times n}$ be the initial input to the transformer blocks.</p> <h3 id=steps-2-6-decoder-block-repeated-l-times>Steps 2-6: Decoder Block (repeated L times)</h3> <p>For block $\ell = 1, \dots, L$, with input $X^{(\ell-1)} \in \mathbb{R}^{d \times n}$:</p> <p><strong>Multi-Head Masked Attention</strong></p> <p>Queries, keys, and values for head $h$:</p> <p>$$Q^{(h)}(x_i) = (W_h^{Q})^T x_i, \quad K^{(h)}(x_i) = (W_h^{K})^T x_i, \quad V^{(h)}(x_i) = (W_h^{V})^T x_i$$</p> <p>where $W_h^{Q}, W_h^{K}, W_h^{V} \in \mathbb{R}^{d \times k}$.</p> <p><strong>Masked Attention Weights</strong></p> <p>$$\alpha_{i,j}^{(h)} = softmax_j \left(\frac{Q^{(h)}(x_i) \cdot K^{(h)}(x_j)}{\sqrt{k}} + M_{i,j}\right)$$</p> <p>where the causal mask $M_{i,j} = 0$ if $j \leq i$, and $M_{i,j} = -\infty$ if $j > i$.</p> <p><strong>Output for Each Head</strong></p> <p>$$u_i^{(h)} = \sum_{j=1}^{i} \alpha_{i,j}^{(h)} V^{(h)}(x_j) \in \mathbb{R}^{k}$$</p> <p><strong>Concatenated Output</strong></p> <p>$$u_i' = \sum_{h=1}^{H} (W_h^{O})^T u_i^{(h)}, \quad W_h^{O} \in \mathbb{R}^{k \times d}$$</p> <p><strong>Residual + LayerNorm</strong></p> <p>$$u_i = \text{LayerNorm}(x_i + u_i'; \gamma_1, \beta_1)$$</p> <h3 id=steps-7-8-feed-forward-network>Steps 7-8: Feed-Forward Network</h3> <p>For each position $i$:</p> <p>$$z_i' = (W_2)^T \text{ReLU}((W_1)^T u_i), \quad W_1 \in \mathbb{R}^{d \times m}, , W_2 \in \mathbb{R}^{m \times d}$$</p> <p><strong>Residual + LayerNorm (Block Output)</strong></p> <p>$$z_i = \text{LayerNorm}(u_i + z_i'; \gamma_2, \beta_2)$$</p> <p>Let $X^{(\ell)} = [z_1, \dots, z_n]$. This becomes the input to block $\ell + 1$.</p> <h3 id=steps-9-10-output-logits-and-probabilities>Steps 9-10: Output Logits and Probabilities</h3> <p>After $L$ blocks, let $Z = X^{(L)}$ be the final representations.</p> <p>$$\text{logits}_i = E z_i + b, \quad E \in \mathbb{R}^{V \times d}, , b \in \mathbb{R}^V$$</p> <p>where $E$ is often tied with input embeddings.</p> <p><strong>Prediction Probabilities</strong></p> <p>$$P(t_{i+1} | t_1, \dots, t_i) = \text{softmax}(\text{logits}_i)$$</p> <p>The output at position $i$ predicts the next token $t_{i+1}$, using only information from tokens $t_1, \dots, t_i$ due to causal masking.</p></details><hr><p><strong>References:</strong><ul><li><a href=https://johnthickstun.com/docs/transformers.pdf>Transformer Notes</a> by John Thickstun<li><a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need</a> (Vaswani et al., 2017)</ul><hr><section class=footnotes><ol class=footnotes-list><li id=fn-1><p>Compaction does happen, but infrequently. <a href=#fr-1-1>↩</a></p><li id=fn-2><p>The diagram shows the original encoder-decoder architecture. Modern LLMs like GPT and Claude are <em>decoder-only</em>: they omit the left side (encoder) and the middle "Multi-Head Attention" that attends to encoder outputs. <a href=#fr-2-1>↩</a></p></ol></section></section><p class=credit><em>This post was written in collaboration with <a href=https://claude.ai>Claude</a>.</em><form action=https://buttondown.com/api/emails/embed-subscribe/vinidlidoo class=embeddable-buttondown-form method=post target=_blank><label for=bd-email>Get notified when I post something new.</label><input id=bd-email name=email placeholder=your@email.com type=email><input type=submit value=Subscribe></form></article></main><link href=https://vinidlidoo.github.io/katex.min.css rel=stylesheet><script defer src=https://vinidlidoo.github.io/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://vinidlidoo.github.io/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://vinidlidoo.github.io/atom.xml> <img alt=feed loading=lazy src=https://vinidlidoo.github.io/social_icons/rss.svg title=feed> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://github.com/vinidlidoo> <img alt=github loading=lazy src=https://vinidlidoo.github.io/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://x.com/vinidlidoo> <img alt=x loading=lazy src=https://vinidlidoo.github.io/social_icons/x.svg title=x> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> <p><p>© 2026 Vincent Ethier</p> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> 1 result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>